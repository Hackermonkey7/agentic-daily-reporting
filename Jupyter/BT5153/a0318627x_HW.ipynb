{"cells":[{"cell_type":"markdown","metadata":{"id":"eNbH3R3BA-K_"},"source":["## Homework 2: Prompting Techniques\n","\n","You will practice prompting techniques by crafting prompts to complete specific tasks. Each task's instructions are at the top of its corresponding cell.\n","\n","##### Submission Instructions\n","1. Complete all the functions marked with \"Your code here\".\n","2. Run all cells to make sure your outputs are correct.\n","3. Answer all output questions in markdown.\n","4. Due on 27th Feb. 2026, Friday at 23:59pm\n","5. Submit your solution notebook to \"Canvas -> HW2-Assignment\"\n","6. **Prepend your NUS userID to the filename, i.e., \"`a0123456b_HW.ipynb`\"**. Only ipython notebooks are accepted.\n","\n","### Installation\n","Make sure you have the required packages installed:\n","\n","```bash\n","pip install langchain langchain-ollama python-dotenv\n","```\n","\n","### Ollama Setup\n","We will be using Ollama with LangChain to run the Qwen 2.5 3B model locally.\n","\n","1. Install Ollama:\n","   - macOS (Homebrew): `brew install --cask ollama`\n","   - Linux: `curl -fsSL https://ollama.com/install.sh | sh`\n","   - Windows: Download from [ollama.com/download](https://ollama.com/download)\n","\n","2. Start Ollama service:\n","   ```bash\n","   ollama serve\n","   ```\n","\n","3. Pull the Qwen 2.5 3B model:\n","   ```bash\n","   ollama pull qwen2.5:3b\n","   ```\n","\n","### Deliverables\n","- Read the task description in each section.\n","- Design and run prompts in the designated cells.\n","- Iterate to improve results until the test script passes.\n","- Save your final prompt(s) and output for each technique.\n","\n","### Evaluation rubric (10 pts total)\n","- 10 for completing the K-shot prompting tasks and output questions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ayUDigOWA-LC"},"outputs":[],"source":["model_card = \"qwen2.5:3b\""]},{"cell_type":"markdown","metadata":{"id":"9K4ly0Z3A-LD"},"source":["## Part 1: K-shot Prompting (10 pts)\n","In this section, you will craft prompts that force strict output formats. Use the examples to guide behavior.\n","\n","### Task 1.1: Structured JSON Extraction\n","Extract and format product information from text into a structured JSON format."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PNe_1FDkA-LD"},"outputs":[],"source":["import os\n","import json\n","from dotenv import load_dotenv\n","from langchain_ollama import ChatOllama\n","\n","load_dotenv()\n","\n","NUM_RUNS_TIMES = 5\n","\n","YOUR_SYSTEM_PROMPT = \"\"\"\n","\"\"\"\n","USER_PROMPT = \"\"\"\n","Extract product information and format as JSON:\n","\n","The iPhone 15 Pro is priced at $999 and comes in 3 colors: titanium, black, and white. It has a 6.1 inch display.\n","\"\"\"\n","\n","EXPECTED_OUTPUT = '{\"name\": \"iPhone 15 Pro\", \"price\": 999, \"colors\": [\"titanium\", \"black\", \"white\"], \"display\": \"6.1 inch\"}'\n","\n","\n","def test_your_prompt(system_prompt: str) -> bool:\n","    \"\"\"Run the prompt up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.\n","\n","    Prints \"SUCCESS\" when a match is found.\n","    \"\"\"\n","    # Initialize ChatOllama\n","    llm = ChatOllama(\n","        model=model_card,\n","        temperature=0.2,\n","    )\n","\n","    for idx in range(NUM_RUNS_TIMES):\n","        print(f\"Running test {idx + 1} of {NUM_RUNS_TIMES}\")\n","\n","        # Create messages with system and user prompts\n","        messages = [\n","            (\"system\", system_prompt),\n","            (\"human\", USER_PROMPT),\n","        ]\n","\n","        response = llm.invoke(messages)\n","        output_text = response.content.strip()\n","\n","        # Try to parse both as JSON and compare\n","        try:\n","            output_json = json.loads(output_text)\n","            expected_json = json.loads(EXPECTED_OUTPUT)\n","            if output_json == expected_json:\n","                print(\"SUCCESS\")\n","                print(f\"Output: {output_text}\")\n","                return True\n","        except json.JSONDecodeError:\n","            pass\n","\n","        print(f\"Expected output: {EXPECTED_OUTPUT}\")\n","        print(f\"Actual output: {output_text}\")\n","    return False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nj5h1KOwA-LE"},"outputs":[],"source":["test_your_prompt(YOUR_SYSTEM_PROMPT)\n"]},{"cell_type":"markdown","metadata":{"id":"EC_CPjnFA-LE"},"source":["### Task 1.2: Reverse a Word\n","Reverse the order of letters in a word. Only output the reversed word, no other text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xW8HZ_mKA-LE"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","from langchain_ollama import ChatOllama\n","\n","load_dotenv()\n","\n","NUM_RUNS_TIMES = 5\n","\n","YOUR_SYSTEM_PROMPT = \"\"\"\n","\"\"\"\n","USER_PROMPT = \"\"\"\n","Reverse the order of letters in the following word. Only output the reversed word, no other text:\n","\n","lexical\n","\"\"\"\n","\n","EXPECTED_OUTPUT = \"lacixel\"\n","\n","\n","def test_your_prompt(system_prompt: str) -> bool:\n","    \"\"\"Run the prompt up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.\n","\n","    Prints \"SUCCESS\" when a match is found.\n","    \"\"\"\n","    # Initialize ChatOllama\n","    llm = ChatOllama(\n","        model=model_card,\n","        temperature=0.2,\n","    )\n","\n","    for idx in range(NUM_RUNS_TIMES):\n","        print(f\"Running test {idx + 1} of {NUM_RUNS_TIMES}\")\n","\n","        # Create messages with system and user prompts\n","        messages = [\n","            (\"system\", system_prompt),\n","            (\"human\", USER_PROMPT),\n","        ]\n","\n","        response = llm.invoke(messages)\n","        output_text = response.content.strip()\n","\n","        if output_text.strip() == EXPECTED_OUTPUT.strip():\n","            print(\"SUCCESS\")\n","            print(f\"Output: {output_text}\")\n","            return True\n","        else:\n","            print(f\"Expected output: {EXPECTED_OUTPUT}\")\n","            print(f\"Actual output: {output_text}\")\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5HRrw9jA-LF","outputId":"8ff35528-c3e2-4c77-8112-d73a580bb689"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running test 1 of 5\n","Expected output: lacixel\n","Actual output: lacixeL\n","Running test 2 of 5\n","Expected output: lacixel\n","Actual output: lacixeL\n","Running test 3 of 5\n","Expected output: lacixel\n","Actual output: ylacexl\n","Running test 4 of 5\n","Expected output: lacixel\n","Actual output: ylacxeL\n","Running test 5 of 5\n","Expected output: lacixel\n","Actual output: ylacexl\n"]},{"data":{"text/plain":["False"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["test_your_prompt(YOUR_SYSTEM_PROMPT)"]},{"cell_type":"markdown","metadata":{"id":"3PNPc1p2A-LF"},"source":["### Task 1.3: Email Subject Classification\n","Classify an email subject into one of: billing, support, sales. Output only the label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWTYvj_FA-LG"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","from langchain_ollama import ChatOllama\n","\n","load_dotenv()\n","\n","NUM_RUNS_TIMES = 5\n","\n","YOUR_SYSTEM_PROMPT = \"\"\"\n","\"\"\"\n","\n","USER_PROMPT = \"\"\"\n","Subject: Invoice overdue for account 4521\n","\"\"\"\n","\n","EXPECTED_OUTPUT = \"billing\"\n","\n","\n","def test_your_prompt(system_prompt: str) -> bool:\n","    \"\"\"Run the prompt up to NUM_RUNS_TIMES and return True if any output matches EXPECTED_OUTPUT.\n","\n","    Prints \"SUCCESS\" when a match is found.\n","    \"\"\"\n","    # Initialize ChatOllama\n","    llm = ChatOllama(\n","        model=model_card,\n","        temperature=0.2,\n","    )\n","\n","    for idx in range(NUM_RUNS_TIMES):\n","        print(f\"Running test {idx + 1} of {NUM_RUNS_TIMES}\")\n","\n","        messages = [\n","            (\"system\", system_prompt),\n","            (\"human\", USER_PROMPT),\n","        ]\n","\n","        response = llm.invoke(messages)\n","        output_text = response.content.strip().lower()\n","\n","        if output_text == EXPECTED_OUTPUT:\n","            print(\"SUCCESS\")\n","            print(f\"Output: {output_text}\")\n","            return True\n","        else:\n","            print(f\"Expected output: {EXPECTED_OUTPUT}\")\n","            print(f\"Actual output: {output_text}\")\n","    return False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbVE5H2RA-LG","outputId":"7c829e4c-b6f0-4544-abb6-24bf16c594e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running test 1 of 5\n","SUCCESS\n","Output: billing\n"]},{"data":{"text/plain":["True"]},"execution_count":null,"metadata":{},"output_type":"execute_result"}],"source":["test_your_prompt(YOUR_SYSTEM_PROMPT)"]},{"cell_type":"markdown","metadata":{"id":"vsx1GZiMA-LG"},"source":["### Task 1.4: Output Questions\n","Answer the following based on the outputs you observe:\n","1. Which k-shot task shows the biggest improvement from adding examples? Why?\n","2. For the JSON task, what kinds of errors do you see without strong prompts (extra keys, formatting, types)?\n","3. Compare a small model vs. a larger model (from OpenAI or Genimi). Does the larger model still benefit from k-shot examples?\n","4. Which examples seem to help the most, and how would you refine them?"]},{"cell_type":"markdown","metadata":{"id":"Ub5s2ZqcA-LG"},"source":["##### your answer here"]},{"cell_type":"markdown","metadata":{"id":"TefWF8ofA-LG"},"source":["#### End of HW"]}],"metadata":{"kernelspec":{"display_name":"bt5153env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}